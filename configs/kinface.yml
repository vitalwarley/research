seed_everything: 100
ckpt_path: null
trainer:
  limit_train_batches: 1.0
  accumulate_grad_batches: 1
  num_sanity_val_steps: 1
  log_every_n_steps: 10
  accelerator: "gpu"
  deterministic: yes
  fast_dev_run: no
  max_epochs: 50
  default_root_dir: exp/
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: exp/checkpoints/
        filename: '{epoch}-{loss/val:.3f}-{loss/train:.3f}-{auc:.6f}'
        monitor: auc
        verbose: no
        save_last: yes
        save_top_k: 1
        save_weights_only: no
        auto_insert_metric_name: no
        mode: max
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: 'auc'  # Replace 'auc' with your chosen metric
        patience: 5  # Number of epochs to wait for improvement
        verbose: False
        mode: 'max'  # Use 'min' if the metric should decrease

data:
  class_path: datasets.kinface.KinFaceWDataModuleKFold
  init_args:
    root_dir: data/kinfacew
    dataset: "I"  # or "II"
    batch_size: 16
    num_workers: 4
    fold: 1

model:
  class_path: models.facornet.FaCoRNet
  init_args:
    optimizer: SGD
    adamw_beta1: 0.9
    adamw_beta2: 0.999
    lr: 1e-3
    momentum: 0.9
    weight_decay: 0
    start_lr: 1e-10
    end_lr: 1e-10
    lr_factor: 0.75
    lr_steps: [8, 14, 25, 35, 40, 50, 60]
    warmup: 200
    cooldown: 400
    scheduler: null
    threshold: null
    anneal_strategy: cos
    weights: null
    model:
      class_path: models.facornet.FaCoR
      init_args:
        model: adaface_ir_101
        attention: models.attention.FaCoRAttention
    loss: 
      class_path: losses.facornet.FaCoRNetCL
      init_args:
        s: 500
